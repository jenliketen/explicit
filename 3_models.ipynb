{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP: CLASSIFICATION OF SONG LYRICS WITH EXPLICIT CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, here is our dataset, fully cleaned, lemmatized, and including word count and sentiment metrics. First, we will vectorize the words using TF-IDF, so that the vectors are normalized against document frequency. We then join these vectors with the other numeric columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from src.pre_model import *\n",
    "from src.models import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>explicit_label</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>lemma_str</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andante, Andante</td>\n",
       "      <td>0</td>\n",
       "      <td>[take, easy, please, touch, gently, summer, ev...</td>\n",
       "      <td>[thousand, butterfly, slow, night, soul, body,...</td>\n",
       "      <td>119</td>\n",
       "      <td>41</td>\n",
       "      <td>take easy please touch gently summer even bree...</td>\n",
       "      <td>0.291667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As Good As New</td>\n",
       "      <td>0</td>\n",
       "      <td>[never, know, go, put, lousy, rotten, show, bo...</td>\n",
       "      <td>[take, say, found, another, way, know, thank, ...</td>\n",
       "      <td>150</td>\n",
       "      <td>63</td>\n",
       "      <td>never know go put lousy rotten show boy tough ...</td>\n",
       "      <td>0.306381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bang-A-Boomerang</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, somebody, happy, question, give, take, ...</td>\n",
       "      <td>[show, tool, boomerang, throw, found, boom, kn...</td>\n",
       "      <td>132</td>\n",
       "      <td>56</td>\n",
       "      <td>make somebody happy question give take learn s...</td>\n",
       "      <td>0.009459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chiquitita</td>\n",
       "      <td>0</td>\n",
       "      <td>[chiquitita, tell, wrong, enchain, sorrow, eye...</td>\n",
       "      <td>[shoulder, cry, candle, feather, best, way, so...</td>\n",
       "      <td>114</td>\n",
       "      <td>58</td>\n",
       "      <td>chiquitita tell wrong enchain sorrow eye hope ...</td>\n",
       "      <td>-0.019366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dancing Queen</td>\n",
       "      <td>0</td>\n",
       "      <td>[dance, jive, time, life, see, girl, watch, sc...</td>\n",
       "      <td>[light, teaser, leave, night, another, dance, ...</td>\n",
       "      <td>93</td>\n",
       "      <td>48</td>\n",
       "      <td>dance jive time life see girl watch scene digg...</td>\n",
       "      <td>0.226238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               song  explicit_label  \\\n",
       "0  Andante, Andante               0   \n",
       "1    As Good As New               0   \n",
       "2  Bang-A-Boomerang               0   \n",
       "3        Chiquitita               0   \n",
       "4     Dancing Queen               0   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [take, easy, please, touch, gently, summer, ev...   \n",
       "1  [never, know, go, put, lousy, rotten, show, bo...   \n",
       "2  [make, somebody, happy, question, give, take, ...   \n",
       "3  [chiquitita, tell, wrong, enchain, sorrow, eye...   \n",
       "4  [dance, jive, time, life, see, girl, watch, sc...   \n",
       "\n",
       "                                        unique_words  word_count  \\\n",
       "0  [thousand, butterfly, slow, night, soul, body,...         119   \n",
       "1  [take, say, found, another, way, know, thank, ...         150   \n",
       "2  [show, tool, boomerang, throw, found, boom, kn...         132   \n",
       "3  [shoulder, cry, candle, feather, best, way, so...         114   \n",
       "4  [light, teaser, leave, night, another, dance, ...          93   \n",
       "\n",
       "   unique_word_count                                          lemma_str  \\\n",
       "0                 41  take easy please touch gently summer even bree...   \n",
       "1                 63  never know go put lousy rotten show boy tough ...   \n",
       "2                 56  make somebody happy question give take learn s...   \n",
       "3                 58  chiquitita tell wrong enchain sorrow eye hope ...   \n",
       "4                 48  dance jive time life see girl watch scene digg...   \n",
       "\n",
       "   sentiment  \n",
       "0   0.291667  \n",
       "1   0.306381  \n",
       "2   0.009459  \n",
       "3  -0.019366  \n",
       "4   0.226238  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"./data/data.csv\", converters={\"lemmatized\": eval,\n",
    "                                                  \"unique_words\": eval})\n",
    "data.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>explicit_label</th>\n",
       "      <th>abandon</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absurd</th>\n",
       "      <th>...</th>\n",
       "      <th>yonder</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zappa</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>41</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>63</td>\n",
       "      <td>0.306381</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132</td>\n",
       "      <td>56</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>58</td>\n",
       "      <td>-0.019366</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "      <td>48</td>\n",
       "      <td>0.226238</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  unique_word_count  sentiment  explicit_label  abandon  ability  \\\n",
       "0         119                 41   0.291667               0      0.0      0.0   \n",
       "1         150                 63   0.306381               0      0.0      0.0   \n",
       "2         132                 56   0.009459               0      0.0      0.0   \n",
       "3         114                 58  -0.019366               0      0.0      0.0   \n",
       "4          93                 48   0.226238               0      0.0      0.0   \n",
       "\n",
       "   able  aboard  absolutely  absurd  ...  yonder  york     young  youth  \\\n",
       "0   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "1   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "2   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "3   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "4   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.151178    0.0   \n",
       "\n",
       "   zappa  zero  zip  zombie  zone  zoo  \n",
       "0    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "1    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "2    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "3    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "4    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 4284 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec = vectorize(data)\n",
    "data_vec.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expectedly, this new dataset has large dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24482, 4284)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample for class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From EDA, we know that our dataset has a class imbalance issue, with almost 20 times as many non-explicit songs as explicit songs. To combat this, we use upsampling to match number of explicit songs to that of non-explicit songs. We also have an option for downsampling, but that line is commented out as our results will show that up-sampling gives better model performance across most models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>explicit_label</th>\n",
       "      <th>abandon</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>aboard</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absurd</th>\n",
       "      <th>...</th>\n",
       "      <th>yonder</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>zappa</th>\n",
       "      <th>zero</th>\n",
       "      <th>zip</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>119</td>\n",
       "      <td>41</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>150</td>\n",
       "      <td>63</td>\n",
       "      <td>0.306381</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132</td>\n",
       "      <td>56</td>\n",
       "      <td>0.009459</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114</td>\n",
       "      <td>58</td>\n",
       "      <td>-0.019366</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>93</td>\n",
       "      <td>48</td>\n",
       "      <td>0.226238</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4284 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  unique_word_count  sentiment  explicit_label  abandon  ability  \\\n",
       "0         119                 41   0.291667               0      0.0      0.0   \n",
       "1         150                 63   0.306381               0      0.0      0.0   \n",
       "2         132                 56   0.009459               0      0.0      0.0   \n",
       "3         114                 58  -0.019366               0      0.0      0.0   \n",
       "4          93                 48   0.226238               0      0.0      0.0   \n",
       "\n",
       "   able  aboard  absolutely  absurd  ...  yonder  york     young  youth  \\\n",
       "0   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "1   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "2   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "3   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.000000    0.0   \n",
       "4   0.0     0.0         0.0     0.0  ...     0.0   0.0  0.151178    0.0   \n",
       "\n",
       "   zappa  zero  zip  zombie  zone  zoo  \n",
       "0    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "1    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "2    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "3    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "4    0.0   0.0  0.0     0.0   0.0  0.0  \n",
       "\n",
       "[5 rows x 4284 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resampled = resample_data(data_vec, \"up\")\n",
    "# data_resampled = resample_data(data_vec, \"down\")\n",
    "data_resampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23132\n",
       "0    23132\n",
       "Name: explicit_label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_resampled[\"explicit_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into training and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a 75-25 train-test split on nearly 50,000 observations. This ensures that we have a good amount of training data. We only standardize `word_count` and `unique_word_count`, as the TF-IDF vectors are already normalized and sentiment scores all range from -1 to 1.\n",
    "\n",
    "\n",
    "\n",
    "For cross validation, we further divide the training data into 5 folds and repeat the hyperparameter tuning process until each fold has had a chance to be the validation set. No testing observation is exposed to the models during hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(data_resampled, standardize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data has high dimensions. As such, we pick classfication models that work well with this, including penalized logistic regression, linear support vector machine (SVM), random forest, gradient boosting, and na&iuml;ve Bayes. We do not use models such as _k_-nearest neighbors (kNN), decision tree, or unregularized logistic regression, as these tend to be negatively impacted by the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression (with L1 penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a good starting place for a binary classification problem. We apply L1 penalty to combat high dimensionality and perform embedded feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model, lr_train_pred, lr_test_pred = lr(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM tends to do well with high dimensional data, as the classifier only looks at support vectors. We use a linear kernel to preserve model parsimony."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model, svm_train_pred, svm_test_pred = svm(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest is built using the number of trees that produces the best out-of-bag (OOB) score out of a few candidates ranging from 1 to 1,000 trees. We cap the `max_depth` at 20 splits and use the square root of all features at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees: 500\n",
      "Out-of-bag score: 0.9454435414144907\n"
     ]
    }
   ],
   "source": [
    "rf_model, rf_train_pred, rf_test_pred, oob_score, rf_fi = rf(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the random forest, the gradient boost is built using the number of trees that produces the best accuracy out of a few candidates ranging from 1 to 1,000 trees. We cap the `max_depth` at 20 splits and use the square root of all features at any given time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trees: 200\n"
     ]
    }
   ],
   "source": [
    "gb_model, gb_train_pred, gb_test_pred, gb_fi = gb(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Na&iuml;ve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The na&iuml;ve Bayes model assumes conditional independence of the features. This may not always be true for NLP problems, as words tend to have order. However, our vectorization no longer preserves word order and word order is not usually a determining factor for explicit content. Na&iuml;ve Bayes is also very fast to train as it does not require hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model, nb_train_pred, nb_test_pred = nb(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save models and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/X_train.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(X_train, handle)\n",
    "with open(\"./data/X_test.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(X_test, handle)\n",
    "with open(\"./data/y_train.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(y_train, handle)\n",
    "with open(\"./data/y_test.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(y_test, handle)\n",
    "\n",
    "with open(\"./results/models/lr_model.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(lr_model, handle)\n",
    "with open(\"./results/models/svm_model.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(svm_model, handle)    \n",
    "with open(\"./results/models/rf_model.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(rf_model, handle)\n",
    "with open(\"./results/models/gb_model.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(gb_model, handle)\n",
    "with open(\"./results/models/nb_model.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(nb_model, handle)\n",
    "    \n",
    "with open(\"./results/predictions/lr_train_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(lr_train_pred, handle)\n",
    "with open(\"./results/predictions/svm_train_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(svm_train_pred, handle)\n",
    "with open(\"./results/predictions/rf_train_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(rf_train_pred, handle)\n",
    "with open(\"./results/predictions/gb_train_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(gb_train_pred, handle)\n",
    "with open(\"./results/predictions/nb_train_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(nb_train_pred, handle)\n",
    "\n",
    "with open(\"./results/predictions/lr_test_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(lr_test_pred, handle)\n",
    "with open(\"./results/predictions/svm_test_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(svm_test_pred, handle)\n",
    "with open(\"./results/predictions/rf_test_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(rf_test_pred, handle)\n",
    "with open(\"./results/predictions/gb_test_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(gb_test_pred, handle)\n",
    "with open(\"./results/predictions/nb_test_pred.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(nb_test_pred, handle)\n",
    "    \n",
    "with open(\"./results/predictions/rf_fi.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(rf_fi, handle)\n",
    "with open(\"./results/predictions/gb_fi.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(gb_fi, handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
